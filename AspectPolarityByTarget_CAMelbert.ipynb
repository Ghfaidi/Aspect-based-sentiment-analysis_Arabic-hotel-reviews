{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9cd5594042654c8e8f13596858551b14": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_123df5860f5e450e8e3bfe1dcef411a6",
              "IPY_MODEL_44758971c3f747958c6e2c3bd51c5c2b",
              "IPY_MODEL_c9e956b10a1f4ada899d544b25b35ae6"
            ],
            "layout": "IPY_MODEL_e48316fb6fd0478a9783cdf686d95207"
          }
        },
        "123df5860f5e450e8e3bfe1dcef411a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e371ba38b4fb4e948cdefb00ac7fd1be",
            "placeholder": "​",
            "style": "IPY_MODEL_c0309538566c43cebc7fb6192033dbb8",
            "value": "Downloading pytorch_model.bin: 100%"
          }
        },
        "44758971c3f747958c6e2c3bd51c5c2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e1264e139d1443269a78140a7b24a7dc",
            "max": 438873130,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4a78798c83f345a585a58313c75f1acf",
            "value": 438873130
          }
        },
        "c9e956b10a1f4ada899d544b25b35ae6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7f3e9bf4b4ab44ce824ce8a7ccc074dd",
            "placeholder": "​",
            "style": "IPY_MODEL_ce30b40f6a844b7fb0f977b7f5b927ef",
            "value": " 439M/439M [00:04&lt;00:00, 124MB/s]"
          }
        },
        "e48316fb6fd0478a9783cdf686d95207": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e371ba38b4fb4e948cdefb00ac7fd1be": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c0309538566c43cebc7fb6192033dbb8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e1264e139d1443269a78140a7b24a7dc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4a78798c83f345a585a58313c75f1acf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7f3e9bf4b4ab44ce824ce8a7ccc074dd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce30b40f6a844b7fb0f977b7f5b927ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "73b80a1ba4724de2801c30817e780a46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_99d32869a1d64ac1908f525d5129e4d7",
              "IPY_MODEL_a3169ffbddc848d0952a61fd08034301",
              "IPY_MODEL_be98f47a5a624217988edeb1fbbcb2b6"
            ],
            "layout": "IPY_MODEL_316e1b48936045958e19406e6315b32d"
          }
        },
        "99d32869a1d64ac1908f525d5129e4d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_84dfea66a1524e719f01b7b34627b107",
            "placeholder": "​",
            "style": "IPY_MODEL_2260bce7647d496d9d5e70c9ab514e76",
            "value": "Downloading (…)okenizer_config.json: 100%"
          }
        },
        "a3169ffbddc848d0952a61fd08034301": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8e0977ceaef24c9880c49b753e31d89a",
            "max": 86,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f49f09499de247ffa1c547be32f62fc1",
            "value": 86
          }
        },
        "be98f47a5a624217988edeb1fbbcb2b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_23f0914eb0074dcb86a89eb5e8136e3e",
            "placeholder": "​",
            "style": "IPY_MODEL_af0daad360d8429486f03ee63c6dbf32",
            "value": " 86.0/86.0 [00:00&lt;00:00, 4.42kB/s]"
          }
        },
        "316e1b48936045958e19406e6315b32d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "84dfea66a1524e719f01b7b34627b107": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2260bce7647d496d9d5e70c9ab514e76": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8e0977ceaef24c9880c49b753e31d89a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f49f09499de247ffa1c547be32f62fc1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "23f0914eb0074dcb86a89eb5e8136e3e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af0daad360d8429486f03ee63c6dbf32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4a47a2415dae45328258a197f3034037": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_333104daa0e34de69f24355863211956",
              "IPY_MODEL_a2224b186c9c41b397c3752ed4db075e",
              "IPY_MODEL_71f86c72e4634c74a5c4b5b8dadc1449"
            ],
            "layout": "IPY_MODEL_799ebf0fd64f40c79f6dfadb1c627f28"
          }
        },
        "333104daa0e34de69f24355863211956": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0d7955f422a64e93b0266bf4d84c639a",
            "placeholder": "​",
            "style": "IPY_MODEL_ec5cee73b131412082a16260b5f6ae0b",
            "value": "Downloading (…)lve/main/config.json: 100%"
          }
        },
        "a2224b186c9c41b397c3752ed4db075e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_024c4512153d4ff18086d289da33e572",
            "max": 468,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3f852aa7ba1e4d3692f54f964acca818",
            "value": 468
          }
        },
        "71f86c72e4634c74a5c4b5b8dadc1449": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7f34451cc62b4fc3980371be084bb5f6",
            "placeholder": "​",
            "style": "IPY_MODEL_4eb7484811674637b641659c7423323d",
            "value": " 468/468 [00:00&lt;00:00, 17.8kB/s]"
          }
        },
        "799ebf0fd64f40c79f6dfadb1c627f28": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0d7955f422a64e93b0266bf4d84c639a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ec5cee73b131412082a16260b5f6ae0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "024c4512153d4ff18086d289da33e572": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3f852aa7ba1e4d3692f54f964acca818": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7f34451cc62b4fc3980371be084bb5f6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4eb7484811674637b641659c7423323d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fcdbd61ed7e14c65be761d8b8e0d13de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0671185cdb65497c9f48d520aebd8020",
              "IPY_MODEL_25261803a00044fc9114214d5668c514",
              "IPY_MODEL_7672d4eebd20422fb5dcd6f09b2e56cb"
            ],
            "layout": "IPY_MODEL_a293319cd428474ba2ecab63291b9031"
          }
        },
        "0671185cdb65497c9f48d520aebd8020": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_423b00729d05445cadb58be7adc93c3b",
            "placeholder": "​",
            "style": "IPY_MODEL_6067fb737ca34767acc543445827cf30",
            "value": "Downloading (…)solve/main/vocab.txt: 100%"
          }
        },
        "25261803a00044fc9114214d5668c514": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d50fc60b66b84461852ba248fe32e0d3",
            "max": 304561,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_433c151010604cf1afeeb7c69681d96e",
            "value": 304561
          }
        },
        "7672d4eebd20422fb5dcd6f09b2e56cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6ef2c2a752a141bd8a34137e45f91651",
            "placeholder": "​",
            "style": "IPY_MODEL_6f4d605598bc43148bf0a5096c0a61ce",
            "value": " 305k/305k [00:00&lt;00:00, 1.67MB/s]"
          }
        },
        "a293319cd428474ba2ecab63291b9031": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "423b00729d05445cadb58be7adc93c3b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6067fb737ca34767acc543445827cf30": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d50fc60b66b84461852ba248fe32e0d3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "433c151010604cf1afeeb7c69681d96e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6ef2c2a752a141bd8a34137e45f91651": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f4d605598bc43148bf0a5096c0a61ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "685b26504ecd41fa8a89089c5b3b2ed9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_828bd0cdff154ec78f946016641c2b53",
              "IPY_MODEL_4ba7dd8100cd4380a974d90863336c73",
              "IPY_MODEL_4aefaa34eb7047ec8004d4c0444449b0"
            ],
            "layout": "IPY_MODEL_0984970633bc4cdbb46e06792a38ed92"
          }
        },
        "828bd0cdff154ec78f946016641c2b53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_77986438f7fc40578429bac4397ea26a",
            "placeholder": "​",
            "style": "IPY_MODEL_90a1d77567d34bc2bac75e2988c27200",
            "value": "Downloading (…)cial_tokens_map.json: 100%"
          }
        },
        "4ba7dd8100cd4380a974d90863336c73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d3e6eee16b374e56aafa59f78de73c10",
            "max": 112,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_101b80f83cc145439ae23ff235c128b3",
            "value": 112
          }
        },
        "4aefaa34eb7047ec8004d4c0444449b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1984f2ea5ae44439ac8859d8e359dceb",
            "placeholder": "​",
            "style": "IPY_MODEL_713571bc264449a5baa0d631b792f8bf",
            "value": " 112/112 [00:00&lt;00:00, 8.65kB/s]"
          }
        },
        "0984970633bc4cdbb46e06792a38ed92": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "77986438f7fc40578429bac4397ea26a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "90a1d77567d34bc2bac75e2988c27200": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d3e6eee16b374e56aafa59f78de73c10": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "101b80f83cc145439ae23ff235c128b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1984f2ea5ae44439ac8859d8e359dceb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "713571bc264449a5baa0d631b792f8bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jGwFYvTOOYvv",
        "outputId": "9d6030e2-636f-4b28-b7a4-2aeb3ae188f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-lightning\n",
        "!pip install torchmetrics"
      ],
      "metadata": {
        "id": "zRVmX4-fTbMa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c44455d-5ea5-4de8-ac5c-8813e6f620cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pytorch-lightning\n",
            "  Downloading pytorch_lightning-2.0.2-py3-none-any.whl (719 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m719.0/719.0 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (1.22.4)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (2.0.1+cu118)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (4.65.0)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (6.0)\n",
            "Requirement already satisfied: fsspec[http]>2021.06.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (2023.4.0)\n",
            "Collecting torchmetrics>=0.7.0 (from pytorch-lightning)\n",
            "  Downloading torchmetrics-0.11.4-py3-none-any.whl (519 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.2/519.2 kB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=17.1 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (23.1)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (4.5.0)\n",
            "Collecting lightning-utilities>=0.7.0 (from pytorch-lightning)\n",
            "  Downloading lightning_utilities-0.8.0-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>2021.06.0->pytorch-lightning) (2.27.1)\n",
            "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]>2021.06.0->pytorch-lightning)\n",
            "  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m74.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->pytorch-lightning) (3.12.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->pytorch-lightning) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->pytorch-lightning) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->pytorch-lightning) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->pytorch-lightning) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.11.0->pytorch-lightning) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.11.0->pytorch-lightning) (16.0.5)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (2.0.12)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning)\n",
            "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning)\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting yarl<2.0,>=1.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning)\n",
            "  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning)\n",
            "  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning)\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->pytorch-lightning) (2.1.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->pytorch-lightning) (1.3.0)\n",
            "Installing collected packages: multidict, lightning-utilities, frozenlist, async-timeout, yarl, aiosignal, aiohttp, torchmetrics, pytorch-lightning\n",
            "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 frozenlist-1.3.3 lightning-utilities-0.8.0 multidict-6.0.4 pytorch-lightning-2.0.2 torchmetrics-0.11.4 yarl-1.9.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.10/dist-packages (0.11.4)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.22.4)\n",
            "Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.0.1+cu118)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (23.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.1->torchmetrics) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.1->torchmetrics) (16.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.1->torchmetrics) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.1->torchmetrics) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import pytorch_lightning as pl\n",
        "from torchmetrics.functional import auroc, accuracy\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from pytorch_lightning.loggers import TensorBoardLogger"
      ],
      "metadata": {
        "id": "6QJtEFjAtpBp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LR5N-L-Sov5T"
      },
      "outputs": [],
      "source": [
        "class HotelReviewTagger(pl.LightningModule):\n",
        "\n",
        "  def __init__(self, n_classes: int, n_training_steps=None, n_warmup_steps=None):\n",
        "    super().__init__()\n",
        "    self.bert = BertModel.from_pretrained(BERT_MODEL_NAME, return_dict=True)\n",
        "    self.classifier = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
        "    self.dropout = nn.Dropout(0.4)\n",
        "    self.n_training_steps = n_training_steps\n",
        "    self.n_warmup_steps = n_warmup_steps\n",
        "    self.criterion = nn.BCELoss()\n",
        "\n",
        "  def forward(self, input_ids, attention_mask, labels=None):\n",
        "    output = self.bert(input_ids, attention_mask=attention_mask)\n",
        "    output = self.classifier(output.pooler_output)\n",
        "    output = self.dropout(output)\n",
        "    output = torch.sigmoid(output)\n",
        "    loss = 0\n",
        "    if labels is not None:\n",
        "        loss = self.criterion(output, labels)\n",
        "    return loss, output\n",
        "\n",
        "  def training_step(self, batch, batch_idx):\n",
        "    input_ids = batch[\"input_ids\"]\n",
        "    attention_mask = batch[\"attention_mask\"]\n",
        "    labels = batch[\"labels\"]\n",
        "    loss, outputs = self(input_ids, attention_mask, labels)\n",
        "    self.log(\"train_loss\", loss, prog_bar=True, logger=True)\n",
        "    return {\"loss\": loss, \"predictions\": outputs, \"labels\": labels}\n",
        "\n",
        "  def validation_step(self, batch, batch_idx):\n",
        "    input_ids = batch[\"input_ids\"]\n",
        "    attention_mask = batch[\"attention_mask\"]\n",
        "    labels = batch[\"labels\"]\n",
        "    loss = self(input_ids, attention_mask, labels)\n",
        "    self.log(\"val_loss\", loss, prog_bar=True, logger=True)\n",
        "    return loss\n",
        "\n",
        "  def test_step(self, batch, batch_idx):\n",
        "    input_ids = batch[\"input_ids\"]\n",
        "    attention_mask = batch[\"attention_mask\"]\n",
        "    labels = batch[\"labels\"]\n",
        "    loss, outputs = self(input_ids, attention_mask, labels)\n",
        "    self.log(\"test_loss\", loss, prog_bar=True, logger=True)\n",
        "    return loss\n",
        "\n",
        "  def training_epoch_end(self, outputs):\n",
        "\n",
        "    labels = []\n",
        "    predictions = []\n",
        "    for output in outputs:\n",
        "      for out_labels in output[\"labels\"].detach().cpu():\n",
        "        labels.append(out_labels)\n",
        "      for out_predictions in output[\"predictions\"].detach().cpu():\n",
        "        predictions.append(out_predictions)\n",
        "\n",
        "    labels = torch.stack(labels).int()\n",
        "    predictions = torch.stack(predictions)\n",
        "\n",
        "    for i, name in enumerate(LABEL_COLUMNS):\n",
        "      class_roc_auc = auroc(predictions[:, i], labels[:, i])\n",
        "      self.logger.experiment.add_scalar(f\"{name}_roc_auc/Train\", class_roc_auc, self.current_epoch)\n",
        "\n",
        "\n",
        "  def configure_optimizers(self):\n",
        "\n",
        "    optimizer = AdamW(self.parameters(), lr=2e-5 , weight_decay=0.001)\n",
        "\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "      optimizer,\n",
        "      num_warmup_steps=self.n_warmup_steps,\n",
        "      num_training_steps=self.n_training_steps\n",
        "    )\n",
        "\n",
        "    return dict(\n",
        "      optimizer=optimizer,\n",
        "      lr_scheduler=dict(\n",
        "        scheduler=scheduler,\n",
        "        interval='step'\n",
        "      )\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "\n",
        "df = pd.read_excel('/content/gdrive/MyDrive/SemEval2016_arabic/SemEval2016_arabic/new_dataset.xlsx')\n",
        "LABEL_COLUMNS = df.columns.tolist()[1:]"
      ],
      "metadata": {
        "id": "jwh0Tr1utzYk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(LABEL_COLUMNS))"
      ],
      "metadata": {
        "id": "v1AGf5JQONCe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3760d91c-9533-428b-d937-6423cca8050f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "NT-hvaJ3uCrP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b5d9a5c-9b24-4268-a128-8765a631494b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.29.2-py3-none-any.whl (7.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m73.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m107.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.15.1 tokenizers-0.13.3 transformers-4.29.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "BERT_MODEL_NAME = \"CAMeL-Lab/bert-base-arabic-camelbert-da\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(BERT_MODEL_NAME)"
      ],
      "metadata": {
        "id": "s1lRzbFOuKtZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145,
          "referenced_widgets": [
            "73b80a1ba4724de2801c30817e780a46",
            "99d32869a1d64ac1908f525d5129e4d7",
            "a3169ffbddc848d0952a61fd08034301",
            "be98f47a5a624217988edeb1fbbcb2b6",
            "316e1b48936045958e19406e6315b32d",
            "84dfea66a1524e719f01b7b34627b107",
            "2260bce7647d496d9d5e70c9ab514e76",
            "8e0977ceaef24c9880c49b753e31d89a",
            "f49f09499de247ffa1c547be32f62fc1",
            "23f0914eb0074dcb86a89eb5e8136e3e",
            "af0daad360d8429486f03ee63c6dbf32",
            "4a47a2415dae45328258a197f3034037",
            "333104daa0e34de69f24355863211956",
            "a2224b186c9c41b397c3752ed4db075e",
            "71f86c72e4634c74a5c4b5b8dadc1449",
            "799ebf0fd64f40c79f6dfadb1c627f28",
            "0d7955f422a64e93b0266bf4d84c639a",
            "ec5cee73b131412082a16260b5f6ae0b",
            "024c4512153d4ff18086d289da33e572",
            "3f852aa7ba1e4d3692f54f964acca818",
            "7f34451cc62b4fc3980371be084bb5f6",
            "4eb7484811674637b641659c7423323d",
            "fcdbd61ed7e14c65be761d8b8e0d13de",
            "0671185cdb65497c9f48d520aebd8020",
            "25261803a00044fc9114214d5668c514",
            "7672d4eebd20422fb5dcd6f09b2e56cb",
            "a293319cd428474ba2ecab63291b9031",
            "423b00729d05445cadb58be7adc93c3b",
            "6067fb737ca34767acc543445827cf30",
            "d50fc60b66b84461852ba248fe32e0d3",
            "433c151010604cf1afeeb7c69681d96e",
            "6ef2c2a752a141bd8a34137e45f91651",
            "6f4d605598bc43148bf0a5096c0a61ce",
            "685b26504ecd41fa8a89089c5b3b2ed9",
            "828bd0cdff154ec78f946016641c2b53",
            "4ba7dd8100cd4380a974d90863336c73",
            "4aefaa34eb7047ec8004d4c0444449b0",
            "0984970633bc4cdbb46e06792a38ed92",
            "77986438f7fc40578429bac4397ea26a",
            "90a1d77567d34bc2bac75e2988c27200",
            "d3e6eee16b374e56aafa59f78de73c10",
            "101b80f83cc145439ae23ff235c128b3",
            "1984f2ea5ae44439ac8859d8e359dceb",
            "713571bc264449a5baa0d631b792f8bf"
          ]
        },
        "outputId": "ecfc28fb-fcd3-4f70-8490-cda111952bf1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)okenizer_config.json:   0%|          | 0.00/86.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "73b80a1ba4724de2801c30817e780a46"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/468 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4a47a2415dae45328258a197f3034037"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/305k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fcdbd61ed7e14c65be761d8b8e0d13de"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "685b26504ecd41fa8a89089c5b3b2ed9"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertModel\n",
        "\n",
        "bert_model = BertModel.from_pretrained(BERT_MODEL_NAME, return_dict=True)"
      ],
      "metadata": {
        "id": "89ZwPN7PuCJ6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121,
          "referenced_widgets": [
            "9cd5594042654c8e8f13596858551b14",
            "123df5860f5e450e8e3bfe1dcef411a6",
            "44758971c3f747958c6e2c3bd51c5c2b",
            "c9e956b10a1f4ada899d544b25b35ae6",
            "e48316fb6fd0478a9783cdf686d95207",
            "e371ba38b4fb4e948cdefb00ac7fd1be",
            "c0309538566c43cebc7fb6192033dbb8",
            "e1264e139d1443269a78140a7b24a7dc",
            "4a78798c83f345a585a58313c75f1acf",
            "7f3e9bf4b4ab44ce824ce8a7ccc074dd",
            "ce30b40f6a844b7fb0f977b7f5b927ef"
          ]
        },
        "outputId": "e3ab49ee-0dd8-4fb0-d603-6d7bb7fe9660"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/439M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9cd5594042654c8e8f13596858551b14"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at CAMeL-Lab/bert-base-arabic-camelbert-da were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PHDwRs22E42z"
      },
      "outputs": [],
      "source": [
        "#MODEL_PATH = '/content/drive/MyDrive/hotel-review-epoch9.ckpt'\n",
        "\n",
        "#trained_model = HotelReviewTagger.load_from_checkpoint(MODEL_PATH, n_classes=len(LABEL_COLUMNS))\n",
        "#trained_model.eval()\n",
        "#trained_model.freeze()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#THRESHOLD = 0.4\n",
        "#test_comment = \"الغرف فسيحة ودورات المياة ممتازة وفخمة ومتكاملة ، لم يعجبني المسبح لانه اصغر مما يجب على فندق خمس نجوم وكذلك عدم وجود واي فاي مجانا\"\n",
        "\n",
        "#encoding = tokenizer.encode_plus(\n",
        "# test_comment,\n",
        "# add_special_tokens=True,\n",
        "#max_length=128,\n",
        "# return_token_type_ids=False,\n",
        "# padding=\"max_length\",\n",
        "# return_attention_mask=True,\n",
        "# return_tensors='pt',\n",
        "#)\n",
        "\n",
        "#_, test_prediction = trained_model(encoding[\"input_ids\"], encoding[\"attention_mask\"])\n",
        "#test_prediction = test_prediction.flatten().numpy()\n",
        "#print(encoding)\n",
        "#print(LABEL_COLUMNS)\n",
        "#for label, prediction in zip(LABEL_COLUMNS, test_prediction):\n",
        " # if prediction < THRESHOLD:\n",
        "  #  continue\n",
        "  #print(f\"{label}: {prediction}\")"
      ],
      "metadata": {
        "id": "c-9guuMoWXZC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def to_sentiment(x):\n",
        "  if x == 'positive':\n",
        "    return 2\n",
        "  if x == 'negative':\n",
        "    return 0\n",
        "  if x == 'neutral':\n",
        "    return 1"
      ],
      "metadata": {
        "id": "DnUYmRJ5CehR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aspect_df = pd.read_excel('/content/gdrive/MyDrive/proj/SemEval2016_arabic/SemEval2016_arabic/new_dataset.xlsx')\n",
        "aspect_df = aspect_df.drop(['rid', 'id', 'from', 'to'], axis=1)\n",
        "aspect_df = aspect_df.fillna(method='ffill')"
      ],
      "metadata": {
        "id": "oONiMYJzvQlJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import collections"
      ],
      "metadata": {
        "id": "G9NjTrRU0gwe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# aspect_df[\"polarity\"].count('positive')\n",
        "collections.Counter(aspect_df['polarity'])"
      ],
      "metadata": {
        "id": "Yu7pesPKUMsO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da04bb24-217c-48cc-daf8-f280cd3a989b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({'positive': 6201, 'negative': 3636, 'neutral': 684})"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "aspect_df['sentiment'] = aspect_df.polarity.apply(to_sentiment)\n",
        "aspect_df"
      ],
      "metadata": {
        "id": "xpiaLWVjFvsK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "outputId": "1d7dddd0-7960-4c49-a75e-c05d7f3e3449"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       OutOfScope                                               text  \\\n",
              "0             NaN  نصح نوم تناول طعام موقع مثالي اقامه رحله طيران...   \n",
              "1             NaN  غرفه ممتاز موظف افطار وجبه عشاء مطعم باهظ ثمن ...   \n",
              "2             NaN  غرفه ممتاز موظف افطار وجبه عشاء مطعم باهظ ثمن ...   \n",
              "3             NaN  غرفه ممتاز موظف افطار وجبه عشاء مطعم باهظ ثمن ...   \n",
              "4             NaN  غرفه ممتاز موظف افطار وجبه عشاء مطعم باهظ ثمن ...   \n",
              "...           ...                                                ...   \n",
              "10516         1.0      كثير شاي او قهوه كعك منزل كعك اضافي لمسه راءع   \n",
              "10517         1.0      كثير شاي او قهوه كعك منزل كعك اضافي لمسه راءع   \n",
              "10518         1.0      افضل افطار فندق صغير كثير خيار عدم وجود متاعب   \n",
              "10519         1.0      افضل افطار فندق صغير كثير خيار عدم وجود متاعب   \n",
              "10520         1.0  علي طريق جيد حافل دخول الي مدينه شك شكر جزل تم...   \n",
              "\n",
              "              target                   category  polarity  sentiment  \n",
              "0               موقع           LOCATION#GENERAL  positive          2  \n",
              "1             الغرفة              ROOMS#GENERAL  positive          2  \n",
              "2           الموظفون            SERVICE#GENERAL  positive          2  \n",
              "3      بوفيه الإفطار        FOOD_DRINKS#QUALITY  positive          2  \n",
              "4        وجبة العشاء         FOOD_DRINKS#PRICES  negative          0  \n",
              "...              ...                        ...       ...        ...  \n",
              "10516          الكعك        FOOD_DRINKS#QUALITY  positive          2  \n",
              "10517            كعك        FOOD_DRINKS#QUALITY  positive          2  \n",
              "10518          إفطار  FOOD_DRINKS#STYLE_OPTIONS  positive          2  \n",
              "10519           فندق      HOTEL#DESIGN_FEATURES  positive          2  \n",
              "10520       للحافلات         FACILITIES#GENERAL  positive          2  \n",
              "\n",
              "[10521 rows x 6 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c99aac4f-9ee2-4946-8381-3c7c1ecb9183\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>OutOfScope</th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "      <th>category</th>\n",
              "      <th>polarity</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>نصح نوم تناول طعام موقع مثالي اقامه رحله طيران...</td>\n",
              "      <td>موقع</td>\n",
              "      <td>LOCATION#GENERAL</td>\n",
              "      <td>positive</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>NaN</td>\n",
              "      <td>غرفه ممتاز موظف افطار وجبه عشاء مطعم باهظ ثمن ...</td>\n",
              "      <td>الغرفة</td>\n",
              "      <td>ROOMS#GENERAL</td>\n",
              "      <td>positive</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>NaN</td>\n",
              "      <td>غرفه ممتاز موظف افطار وجبه عشاء مطعم باهظ ثمن ...</td>\n",
              "      <td>الموظفون</td>\n",
              "      <td>SERVICE#GENERAL</td>\n",
              "      <td>positive</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>NaN</td>\n",
              "      <td>غرفه ممتاز موظف افطار وجبه عشاء مطعم باهظ ثمن ...</td>\n",
              "      <td>بوفيه الإفطار</td>\n",
              "      <td>FOOD_DRINKS#QUALITY</td>\n",
              "      <td>positive</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>NaN</td>\n",
              "      <td>غرفه ممتاز موظف افطار وجبه عشاء مطعم باهظ ثمن ...</td>\n",
              "      <td>وجبة العشاء</td>\n",
              "      <td>FOOD_DRINKS#PRICES</td>\n",
              "      <td>negative</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10516</th>\n",
              "      <td>1.0</td>\n",
              "      <td>كثير شاي او قهوه كعك منزل كعك اضافي لمسه راءع</td>\n",
              "      <td>الكعك</td>\n",
              "      <td>FOOD_DRINKS#QUALITY</td>\n",
              "      <td>positive</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10517</th>\n",
              "      <td>1.0</td>\n",
              "      <td>كثير شاي او قهوه كعك منزل كعك اضافي لمسه راءع</td>\n",
              "      <td>كعك</td>\n",
              "      <td>FOOD_DRINKS#QUALITY</td>\n",
              "      <td>positive</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10518</th>\n",
              "      <td>1.0</td>\n",
              "      <td>افضل افطار فندق صغير كثير خيار عدم وجود متاعب</td>\n",
              "      <td>إفطار</td>\n",
              "      <td>FOOD_DRINKS#STYLE_OPTIONS</td>\n",
              "      <td>positive</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10519</th>\n",
              "      <td>1.0</td>\n",
              "      <td>افضل افطار فندق صغير كثير خيار عدم وجود متاعب</td>\n",
              "      <td>فندق</td>\n",
              "      <td>HOTEL#DESIGN_FEATURES</td>\n",
              "      <td>positive</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10520</th>\n",
              "      <td>1.0</td>\n",
              "      <td>علي طريق جيد حافل دخول الي مدينه شك شكر جزل تم...</td>\n",
              "      <td>للحافلات</td>\n",
              "      <td>FACILITIES#GENERAL</td>\n",
              "      <td>positive</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10521 rows × 6 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c99aac4f-9ee2-4946-8381-3c7c1ecb9183')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c99aac4f-9ee2-4946-8381-3c7c1ecb9183 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c99aac4f-9ee2-4946-8381-3c7c1ecb9183');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LEN = 128\n",
        "class_names = ['negative', 'neutral', 'positive']\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "WmhCy_o48a7g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "iooA4HKk8Lk-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train, df_val = train_test_split(aspect_df, test_size=0.1, stratify=aspect_df.polarity , random_state=42)"
      ],
      "metadata": {
        "id": "lWDfKnhNxvoU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class HotelReviewDataset(Dataset):\n",
        "\n",
        "  def __init__(self, reviews, auxilaries, targets, tokenizer, max_len):\n",
        "    self.reviews = reviews\n",
        "    self.auxilaries = auxilaries\n",
        "    self.targets = targets\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_len = max_len\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.reviews)\n",
        "\n",
        "  def __getitem__(self, item):\n",
        "    review = str(self.reviews[item])\n",
        "    auxilary = str(self.auxilaries[item])\n",
        "    target = self.targets[item]\n",
        "\n",
        "    encoding = self.tokenizer.encode_plus(\n",
        "      review,\n",
        "      auxilary,\n",
        "      add_special_tokens=True,\n",
        "      max_length=self.max_len,\n",
        "      return_token_type_ids=True,\n",
        "      pad_to_max_length=True,\n",
        "      return_attention_mask=True,\n",
        "      return_tensors='pt',\n",
        "    )\n",
        "\n",
        "    return {\n",
        "      'review_text': review,\n",
        "      'input_ids': encoding['input_ids'].flatten(),\n",
        "      'auxilary': auxilary,\n",
        "      'token_type_ids': encoding['token_type_ids'].flatten(),\n",
        "      'attention_mask': encoding['attention_mask'].flatten(),\n",
        "      'targets': torch.tensor(target, dtype=torch.long)\n",
        "    }"
      ],
      "metadata": {
        "id": "OpsGiHOH8R1I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
        "  ds = HotelReviewDataset(\n",
        "    reviews=df.text.to_numpy(),\n",
        "    auxilaries=df['target'].to_numpy(),\n",
        "    targets=df.sentiment.to_numpy(),\n",
        "    tokenizer=tokenizer,\n",
        "    max_len=max_len\n",
        "  )\n",
        "\n",
        "  return DataLoader(\n",
        "    ds,\n",
        "    batch_size=batch_size,\n",
        "    num_workers=4\n",
        "  )"
      ],
      "metadata": {
        "id": "a8E3Bi6L8ZVc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 16\n",
        "\n",
        "train_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "val_data_loader = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)"
      ],
      "metadata": {
        "id": "NYsWGu6I8j0R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbfa845e-b5fa-40d2-fcf5-32e6ab72140f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_data_loader)"
      ],
      "metadata": {
        "id": "5UeXMro8J_tj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f23e40d0-8c67-453e-9a13-a43869d0e4ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<torch.utils.data.dataloader.DataLoader object at 0x7fd98fc73790>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SentimentClassifier(nn.Module):\n",
        "\n",
        "  def __init__(self, n_classes):\n",
        "    super(SentimentClassifier, self).__init__()\n",
        "    self.bert = BertModel.from_pretrained(BERT_MODEL_NAME, return_dict=False)\n",
        "    self.drop = nn.Dropout(p=0.3)\n",
        "    self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
        "\n",
        "  def forward(self, input_ids, attention_mask, token_type_ids):\n",
        "    _, pooled_output = self.bert(\n",
        "      input_ids=input_ids,\n",
        "      attention_mask=attention_mask,\n",
        "      token_type_ids=token_type_ids\n",
        "    )\n",
        "    output = self.drop(pooled_output)\n",
        "    return self.out(output)"
      ],
      "metadata": {
        "id": "AZIj7qEb9iwB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = next(iter(train_data_loader))\n",
        "data.keys()"
      ],
      "metadata": {
        "id": "F4K6tftNUYL7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e6140a9-4370-4052-a8fa-6cbaefd78347"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['review_text', 'input_ids', 'auxilary', 'token_type_ids', 'attention_mask', 'targets'])"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = SentimentClassifier(len(class_names))\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "GMztTysrUwYT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a6e5617-248f-4620-b37e-e9fef03f2355"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at CAMeL-Lab/bert-base-arabic-camelbert-da were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = data['input_ids'].to(device)\n",
        "attention_mask = data['attention_mask'].to(device)\n",
        "token_type_ids = data['token_type_ids'].to(device)\n",
        "\n",
        "print(input_ids.shape) # batch size x seq length\n",
        "print(attention_mask.shape) # batch size x seq length\n",
        "print(token_type_ids.shape)"
      ],
      "metadata": {
        "id": "tsv6J83y_d5R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6a35324-5981-4c88-c625-214f0e49fdba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([16, 128])\n",
            "torch.Size([16, 128])\n",
            "torch.Size([16, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "F.softmax(model(input_ids, attention_mask, token_type_ids), dim=1)"
      ],
      "metadata": {
        "id": "MzDWRaRGU22Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b636e5b0-b685-4b18-a416-01b92274f5b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.2611, 0.3607, 0.3782],\n",
              "        [0.2300, 0.2605, 0.5094],\n",
              "        [0.3523, 0.2832, 0.3645],\n",
              "        [0.2667, 0.2889, 0.4444],\n",
              "        [0.2512, 0.3232, 0.4256],\n",
              "        [0.1994, 0.4553, 0.3453],\n",
              "        [0.1451, 0.2940, 0.5609],\n",
              "        [0.1568, 0.4587, 0.3845],\n",
              "        [0.1680, 0.4262, 0.4058],\n",
              "        [0.1700, 0.4184, 0.4116],\n",
              "        [0.1442, 0.3786, 0.4772],\n",
              "        [0.2039, 0.2990, 0.4970],\n",
              "        [0.2470, 0.2768, 0.4762],\n",
              "        [0.2249, 0.3435, 0.4316],\n",
              "        [0.2234, 0.2676, 0.5090],\n",
              "        [0.2334, 0.3136, 0.4530]], device='cuda:0', grad_fn=<SoftmaxBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "ZhP2AuoSH-kg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
        "import numpy as np\n",
        "from collections import defaultdict"
      ],
      "metadata": {
        "id": "ZkVnTLkBIQfI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False,weight_decay=0.001)\n",
        "total_steps = len(train_data_loader) * EPOCHS\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "  optimizer,\n",
        "  num_warmup_steps=0,\n",
        "  num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss().to(device)"
      ],
      "metadata": {
        "id": "Fv5z0P2GH-J_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12770d47-6a70-4161-c0e2-5dec8d96849d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(\n",
        "  model,\n",
        "  data_loader,\n",
        "  loss_fn,\n",
        "  optimizer,\n",
        "  device,\n",
        "  scheduler,\n",
        "  n_examples\n",
        "):\n",
        "  model = model.train()\n",
        "\n",
        "  losses = []\n",
        "  correct_predictions = 0\n",
        "\n",
        "  for d in data_loader:\n",
        "    input_ids = d[\"input_ids\"].to(device)\n",
        "    attention_mask = d[\"attention_mask\"].to(device)\n",
        "    token_type_ids = d[\"token_type_ids\"].to(device)\n",
        "    targets = d[\"targets\"].to(device)\n",
        "\n",
        "    outputs = model(\n",
        "      input_ids=input_ids,\n",
        "      attention_mask=attention_mask,\n",
        "      token_type_ids=token_type_ids\n",
        "    )\n",
        "\n",
        "    _, preds = torch.max(outputs, dim=1)\n",
        "    loss = loss_fn(outputs, targets)\n",
        "\n",
        "    correct_predictions += torch.sum(preds == targets)\n",
        "    losses.append(loss.item())\n",
        "\n",
        "    loss.backward()\n",
        "    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "  return correct_predictions.double() / n_examples, np.mean(losses)"
      ],
      "metadata": {
        "id": "d5EJM8nOISvX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
        "  model = model.eval()\n",
        "\n",
        "  losses = []\n",
        "  correct_predictions = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for d in data_loader:\n",
        "      input_ids = d[\"input_ids\"].to(device)\n",
        "      attention_mask = d[\"attention_mask\"].to(device)\n",
        "      token_type_ids = d[\"token_type_ids\"].to(device)\n",
        "      targets = d[\"targets\"].to(device)\n",
        "\n",
        "      outputs = model(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        token_type_ids=token_type_ids\n",
        "      )\n",
        "      _, preds = torch.max(outputs, dim=1)\n",
        "\n",
        "      loss = loss_fn(outputs, targets)\n",
        "\n",
        "      correct_predictions += torch.sum(preds == targets)\n",
        "      losses.append(loss.item())\n",
        "\n",
        "  return correct_predictions.double() / n_examples, np.mean(losses)"
      ],
      "metadata": {
        "id": "N_4KdUC4IYqf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "history = defaultdict(list)\n",
        "best_accuracy = 0\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "\n",
        "  print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
        "  print('-' * 10)\n",
        "\n",
        "  train_acc, train_loss = train_epoch(\n",
        "    model,\n",
        "    train_data_loader,\n",
        "    loss_fn,\n",
        "    optimizer,\n",
        "    device,\n",
        "    scheduler,\n",
        "    len(df_train)\n",
        "  )\n",
        "\n",
        "  print(f'Train loss {train_loss} accuracy {train_acc}')\n",
        "\n",
        "  val_acc, val_loss = eval_model(\n",
        "    model,\n",
        "    val_data_loader,\n",
        "    loss_fn,\n",
        "    device,\n",
        "    len(df_val)\n",
        "  )\n",
        "\n",
        "  print(f'Val   loss {val_loss} accuracy {val_acc}')\n",
        "  print()\n",
        "\n",
        "  history['train_acc'].append(train_acc)\n",
        "  history['train_loss'].append(train_loss)\n",
        "  history['val_acc'].append(val_acc)\n",
        "  history['val_loss'].append(val_loss)\n",
        "\n",
        "  if val_acc > best_accuracy:\n",
        "    torch.save(model.state_dict(), 'bestcamelbert_model_state.bin')\n",
        "    best_accuracy = val_acc"
      ],
      "metadata": {
        "id": "d_JCoe5QU_Ni",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7486a710-c5f0-4c99-8483-8a6054396900"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.5231028450652957 accuracy 0.816751161808196\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 0.4264822940934788 accuracy 0.845204178537512\n",
            "\n",
            "Epoch 2/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.3621881680839972 accuracy 0.8691381495564005\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 0.4583493434457165 accuracy 0.8480531813865148\n",
            "\n",
            "Epoch 3/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.2938555972178693 accuracy 0.8917405999155048\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 0.563299170215473 accuracy 0.8395061728395062\n",
            "\n",
            "Epoch 4/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.25212017430867245 accuracy 0.9079002957329954\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 0.6817260129998127 accuracy 0.8309591642924977\n",
            "\n",
            "Epoch 5/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.21934991152728972 accuracy 0.9184621884241656\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 0.7345491836807041 accuracy 0.8271604938271605\n",
            "\n",
            "Epoch 6/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.1993297551576885 accuracy 0.9277566539923954\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 0.7895411803866877 accuracy 0.8290598290598291\n",
            "\n",
            "Epoch 7/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.1815791678817658 accuracy 0.9367342627798901\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 0.8060322646898302 accuracy 0.8319088319088319\n",
            "\n",
            "Epoch 8/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.1684381025171771 accuracy 0.9443388255175327\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 0.8449013560841029 accuracy 0.8385565052231719\n",
            "\n",
            "Epoch 9/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.15723028316790225 accuracy 0.9471905365441486\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 0.8974028991704638 accuracy 0.8357075023741691\n",
            "\n",
            "Epoch 10/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.1462418295623384 accuracy 0.9506759611322348\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 0.9251801542528538 accuracy 0.8309591642924977\n",
            "\n",
            "CPU times: user 34min 9s, sys: 24.1 s, total: 34min 33s\n",
            "Wall time: 35min 28s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "SsfKrCEOT4un"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f3ItwZtFQ1yO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "train_acc = np.array([item.cpu() for item in history['train_acc']])\n",
        "val_acc = np.array([item.cpu() for item in history['val_acc']])\n",
        "\n",
        "plt.plot(train_acc, label='train accuracy')\n",
        "plt.plot(val_acc, label='validation accuracy')\n",
        "\n",
        "plt.title('Training history')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.ylim([0, 1])\n",
        "\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RJN0aZe0VRDy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "outputId": "809d8779-c786-4239-eeab-0af7bd1d17e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_df = pd.read_excel(\"/content/gdrive/MyDrive/proj/SemEval2016_arabic/SemEval2016_arabic/testgold.xlsx\")\n",
        "\n",
        "test_df = test_df.drop(['rid', 'id', 'from', 'to'], axis=1)\n",
        "test_df = test_df.fillna(method='ffill')\n",
        "test_df['sentiment'] = test_df.polarity.apply(to_sentiment)\n",
        "test_data_loader = create_data_loader(test_df, tokenizer, MAX_LEN, BATCH_SIZE)"
      ],
      "metadata": {
        "id": "m6uYbhALVZMj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d06363c-2be9-4155-81ec-fc5dda9cacd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_df.shape)"
      ],
      "metadata": {
        "id": "jLAq7anMRTZc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8c620d5-1b15-4b6e-d149-d9005314c68f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2606, 6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_acc, _ = eval_model(\n",
        "  model,\n",
        "  test_data_loader,\n",
        "  loss_fn,\n",
        "  device,\n",
        "  len(test_df)\n",
        ")\n",
        "\n",
        "test_acc.item()"
      ],
      "metadata": {
        "id": "3eSxThjwVig4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2244fab-4591-45cc-8c99-9848442c6aae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8365310821181888"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_predictions(model, data_loader):\n",
        "  model = model.eval()\n",
        "\n",
        "  review_texts = []\n",
        "  auxilary_texts = []\n",
        "  targets = []\n",
        "  predictions = []\n",
        "  prediction_probs = []\n",
        "  real_values = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for d in data_loader:\n",
        "\n",
        "      texts = d[\"review_text\"]\n",
        "      auxilaries = d[\"auxilary\"]\n",
        "      input_ids = d[\"input_ids\"].to(device)\n",
        "      token_type_ids = d[\"token_type_ids\"].to(device)\n",
        "      attention_mask = d[\"attention_mask\"].to(device)\n",
        "      targets = d[\"targets\"].to(device)\n",
        "\n",
        "      outputs = model(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        token_type_ids=token_type_ids\n",
        "      )\n",
        "      _, preds = torch.max(outputs, dim=1)\n",
        "\n",
        "      probs = F.softmax(outputs, dim=1)\n",
        "\n",
        "      review_texts.extend(texts)\n",
        "      auxilary_texts.extend(auxilaries)\n",
        "      predictions.extend(preds)\n",
        "      prediction_probs.extend(probs)\n",
        "      real_values.extend(targets)\n",
        "\n",
        "  predictions = torch.stack(predictions).cpu()\n",
        "  prediction_probs = torch.stack(prediction_probs).cpu()\n",
        "  real_values = torch.stack(real_values).cpu()\n",
        "  return review_texts, auxilary_texts, predictions, prediction_probs, real_values"
      ],
      "metadata": {
        "id": "QHIIytd7ftNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_review_texts,y_auxilary_texts, y_pred, y_pred_probs, y_test = get_predictions(\n",
        "  model,\n",
        "  test_data_loader\n",
        ")"
      ],
      "metadata": {
        "id": "Dbej_HiMVxtg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1147a7cf-c4d4-4144-fe45-aa83be730863"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(y_test, y_pred, target_names=class_names))"
      ],
      "metadata": {
        "id": "7WRWB10gV0vj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87637daa-f06e-4258-b436-d0424a14ff75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.79      0.89      0.84       927\n",
            "     neutral       0.45      0.12      0.19       169\n",
            "    positive       0.88      0.88      0.88      1510\n",
            "\n",
            "    accuracy                           0.84      2606\n",
            "   macro avg       0.71      0.63      0.64      2606\n",
            "weighted avg       0.82      0.84      0.82      2606\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Plot the confusion matrix heatmap\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "plt.xlabel(\"Predicted labels\")\n",
        "plt.ylabel(\"True labels\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "noOtG34AJpQY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 718
        },
        "outputId": "6d6555a4-5bb7-40af-b980-435cc1befd21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x800 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prediction"
      ],
      "metadata": {
        "id": "OiUcQ7GsgaXO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "review_text = \" كانت الغرفة ممتازة وكذلك الموظفون وبوفيه الإفطار. ومع ذلك فقد كانت وجبة العشاء في المطعم باهظة الثمن وغير مرضية\"\n",
        "target='وجبة العشاء'\n",
        "encoded_review = tokenizer.encode_plus(\n",
        "  review_text,\n",
        "  target,\n",
        "  max_length=MAX_LEN,\n",
        "  add_special_tokens=True,\n",
        "  return_token_type_ids=True,\n",
        "  pad_to_max_length=True,\n",
        "  return_attention_mask=True,\n",
        "  return_tensors='pt',\n",
        ")\n",
        "\n",
        "encoding = tokenizer.encode_plus(\n",
        "  review_text,\n",
        "  add_special_tokens=True,\n",
        "  max_length=128,\n",
        "  return_token_type_ids=False,\n",
        "  padding=\"max_length\",\n",
        "  return_attention_mask=True,\n",
        "  return_tensors='pt',\n",
        ")\n",
        "\n",
        "#_, test_prediction = trained_model(encoding[\"input_ids\"], encoding[\"attention_mask\"])\n",
        "#test_prediction = test_prediction.flatten().numpy()\n",
        "\n",
        "input_ids = encoded_review['input_ids'].to(device)\n",
        "attention_mask = encoded_review['attention_mask'].to(device)\n",
        "token_type_ids = encoded_review['token_type_ids'].to(device)\n",
        "\n",
        "output = model(input_ids, attention_mask, token_type_ids)\n",
        "_, prediction = torch.max(output, dim=1)\n",
        "\n",
        "print(f'Review text: {review_text}')\n",
        "print(f'target text: {target}')\n",
        "print(f'Sentiment  : {class_names[prediction]}')\n",
        "print(f'Labels  : ')\n",
        "#for label, prediction in zip(LABEL_COLUMNS, test_prediction):\n",
        "#  if prediction < THRESHOLD:\n",
        "#    continue\n",
        " # print(f\"{label}: {prediction}\")"
      ],
      "metadata": {
        "id": "om8Fo_STgZ_m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d52eb2a-aca4-4612-d682-3fd7363ab138"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Review text:  كانت الغرفة ممتازة وكذلك الموظفون وبوفيه الإفطار. ومع ذلك فقد كانت وجبة العشاء في المطعم باهظة الثمن وغير مرضية\n",
            "target text: وجبة العشاء\n",
            "Sentiment  : negative\n",
            "Labels  : \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model2 = SentimentClassifier(len(class_names))\n",
        "model2.load_state_dict(torch.load('/content/bestcamelbert_model_state.bin',  map_location=torch.device('cpu')))\n",
        "model2.eval()\n",
        "model2.to(device)"
      ],
      "metadata": {
        "id": "nM1BC3Q5P-Gn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab8276ec-8107-449e-8829-2ecaeda3919c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at CAMeL-Lab/bert-base-arabic-camelbert-da were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SentimentClassifier(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30000, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (drop): Dropout(p=0.3, inplace=False)\n",
              "  (out): Linear(in_features=768, out_features=3, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "review_text = \"بالنسبة لتناول الطعام بالخارج - لا تذهب إلى تاج محل - طعام متوسط المستوى، وخدمة سيئة جدًا.\"\n",
        "target='خدمة'\n",
        "#['لموظفون','SERVICE#GENERAL']\n",
        "# target='الموظفون'\n",
        "#category='SERVICE#GENERAL'\n",
        "encoded_review = tokenizer.encode_plus(\n",
        "  review_text,\n",
        "  target,\n",
        "  max_length=MAX_LEN,\n",
        "  add_special_tokens=True,\n",
        "  return_token_type_ids=True,\n",
        "  pad_to_max_length=True,\n",
        "  return_attention_mask=True,\n",
        "  return_tensors='pt',\n",
        ")\n",
        "\n",
        "input_ids = encoded_review['input_ids'].to(device)\n",
        "attention_mask = encoded_review['attention_mask'].to(device)\n",
        "token_type_ids = encoded_review['token_type_ids'].to(device)\n",
        "\n",
        "output = model2(input_ids, attention_mask, token_type_ids)\n",
        "_, prediction = torch.max(output, dim=1)\n",
        "\n",
        "print(f'Review text: {review_text}')\n",
        "print(f'target text: {target}')\n",
        "print(f'Sentiment  : {class_names[prediction]}')"
      ],
      "metadata": {
        "id": "K2D0r7ePU1gD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52b51b0d-34df-42ca-ab55-317e76385925"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Review text: بالنسبة لتناول الطعام بالخارج - لا تذهب إلى تاج محل - طعام متوسط المستوى، وخدمة سيئة جدًا.\n",
            "target text: خدمة\n",
            "Sentiment  : negative\n"
          ]
        }
      ]
    }
  ]
}