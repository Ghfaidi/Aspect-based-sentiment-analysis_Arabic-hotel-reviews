{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing Arabic dataset"
      ],
      "metadata": {
        "id": "kbqsSRR-EjSC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CDassztUxIQ1",
        "outputId": "02f317c8-e14b-485a-db87-b37d6e94e1f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_excel('/content/gdrive/MyDrive/preproc/trainsb1.xlsx')\n",
        "\n"
      ],
      "metadata": {
        "id": "L0I7OYKzxkvw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2b95a6a2"
      },
      "outputs": [],
      "source": [
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c8f4b1b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53e4518d-ad7b-40e1-a1e5-22e5ec779bd9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "15fea0ec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce616048-b0b7-4364-daac-e4a93183b8ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: farasapy in /usr/local/lib/python3.10/dist-packages (0.0.14)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from farasapy) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from farasapy) (4.65.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->farasapy) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->farasapy) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->farasapy) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->farasapy) (3.4)\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "stopwords_list = stopwords.words('arabic')\n",
        "\n",
        "!pip install farasapy\n",
        "\n",
        "from farasa.stemmer import FarasaStemmer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(stopwords_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sInM6_JpWSlf",
        "outputId": "838cee0c-3184-46c9-d2a2-8eeeb7155556"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['إذ', 'إذا', 'إذما', 'إذن', 'أف', 'أقل', 'أكثر', 'ألا', 'إلا', 'التي', 'الذي', 'الذين', 'اللاتي', 'اللائي', 'اللتان', 'اللتيا', 'اللتين', 'اللذان', 'اللذين', 'اللواتي', 'إلى', 'إليك', 'إليكم', 'إليكما', 'إليكن', 'أم', 'أما', 'أما', 'إما', 'أن', 'إن', 'إنا', 'أنا', 'أنت', 'أنتم', 'أنتما', 'أنتن', 'إنما', 'إنه', 'أنى', 'أنى', 'آه', 'آها', 'أو', 'أولاء', 'أولئك', 'أوه', 'آي', 'أي', 'أيها', 'إي', 'أين', 'أين', 'أينما', 'إيه', 'بخ', 'بس', 'بعد', 'بعض', 'بك', 'بكم', 'بكم', 'بكما', 'بكن', 'بل', 'بلى', 'بما', 'بماذا', 'بمن', 'بنا', 'به', 'بها', 'بهم', 'بهما', 'بهن', 'بي', 'بين', 'بيد', 'تلك', 'تلكم', 'تلكما', 'ته', 'تي', 'تين', 'تينك', 'ثم', 'ثمة', 'حاشا', 'حبذا', 'حتى', 'حيث', 'حيثما', 'حين', 'خلا', 'دون', 'ذا', 'ذات', 'ذاك', 'ذان', 'ذانك', 'ذلك', 'ذلكم', 'ذلكما', 'ذلكن', 'ذه', 'ذو', 'ذوا', 'ذواتا', 'ذواتي', 'ذي', 'ذين', 'ذينك', 'ريث', 'سوف', 'سوى', 'شتان', 'عدا', 'عسى', 'عل', 'على', 'عليك', 'عليه', 'عما', 'عن', 'عند', 'غير', 'فإذا', 'فإن', 'فلا', 'فمن', 'في', 'فيم', 'فيما', 'فيه', 'فيها', 'قد', 'كأن', 'كأنما', 'كأي', 'كأين', 'كذا', 'كذلك', 'كل', 'كلا', 'كلاهما', 'كلتا', 'كلما', 'كليكما', 'كليهما', 'كم', 'كم', 'كما', 'كي', 'كيت', 'كيف', 'كيفما', 'لا', 'لاسيما', 'لدى', 'لست', 'لستم', 'لستما', 'لستن', 'لسن', 'لسنا', 'لعل', 'لك', 'لكم', 'لكما', 'لكن', 'لكنما', 'لكي', 'لكيلا', 'لم', 'لما', 'لن', 'لنا', 'له', 'لها', 'لهم', 'لهما', 'لهن', 'لو', 'لولا', 'لوما', 'لي', 'لئن', 'ليت', 'ليس', 'ليسا', 'ليست', 'ليستا', 'ليسوا', 'ما', 'ماذا', 'متى', 'مذ', 'مع', 'مما', 'ممن', 'من', 'منه', 'منها', 'منذ', 'مه', 'مهما', 'نحن', 'نحو', 'نعم', 'ها', 'هاتان', 'هاته', 'هاتي', 'هاتين', 'هاك', 'هاهنا', 'هذا', 'هذان', 'هذه', 'هذي', 'هذين', 'هكذا', 'هل', 'هلا', 'هم', 'هما', 'هن', 'هنا', 'هناك', 'هنالك', 'هو', 'هؤلاء', 'هي', 'هيا', 'هيت', 'هيهات', 'والذي', 'والذين', 'وإذ', 'وإذا', 'وإن', 'ولا', 'ولكن', 'ولو', 'وما', 'ومن', 'وهو', 'يا', 'أبٌ', 'أخٌ', 'حمٌ', 'فو', 'أنتِ', 'يناير', 'فبراير', 'مارس', 'أبريل', 'مايو', 'يونيو', 'يوليو', 'أغسطس', 'سبتمبر', 'أكتوبر', 'نوفمبر', 'ديسمبر', 'جانفي', 'فيفري', 'مارس', 'أفريل', 'ماي', 'جوان', 'جويلية', 'أوت', 'كانون', 'شباط', 'آذار', 'نيسان', 'أيار', 'حزيران', 'تموز', 'آب', 'أيلول', 'تشرين', 'دولار', 'دينار', 'ريال', 'درهم', 'ليرة', 'جنيه', 'قرش', 'مليم', 'فلس', 'هللة', 'سنتيم', 'يورو', 'ين', 'يوان', 'شيكل', 'واحد', 'اثنان', 'ثلاثة', 'أربعة', 'خمسة', 'ستة', 'سبعة', 'ثمانية', 'تسعة', 'عشرة', 'أحد', 'اثنا', 'اثني', 'إحدى', 'ثلاث', 'أربع', 'خمس', 'ست', 'سبع', 'ثماني', 'تسع', 'عشر', 'ثمان', 'سبت', 'أحد', 'اثنين', 'ثلاثاء', 'أربعاء', 'خميس', 'جمعة', 'أول', 'ثان', 'ثاني', 'ثالث', 'رابع', 'خامس', 'سادس', 'سابع', 'ثامن', 'تاسع', 'عاشر', 'حادي', 'أ', 'ب', 'ت', 'ث', 'ج', 'ح', 'خ', 'د', 'ذ', 'ر', 'ز', 'س', 'ش', 'ص', 'ض', 'ط', 'ظ', 'ع', 'غ', 'ف', 'ق', 'ك', 'ل', 'م', 'ن', 'ه', 'و', 'ي', 'ء', 'ى', 'آ', 'ؤ', 'ئ', 'أ', 'ة', 'ألف', 'باء', 'تاء', 'ثاء', 'جيم', 'حاء', 'خاء', 'دال', 'ذال', 'راء', 'زاي', 'سين', 'شين', 'صاد', 'ضاد', 'طاء', 'ظاء', 'عين', 'غين', 'فاء', 'قاف', 'كاف', 'لام', 'ميم', 'نون', 'هاء', 'واو', 'ياء', 'همزة', 'ي', 'نا', 'ك', 'كن', 'ه', 'إياه', 'إياها', 'إياهما', 'إياهم', 'إياهن', 'إياك', 'إياكما', 'إياكم', 'إياك', 'إياكن', 'إياي', 'إيانا', 'أولالك', 'تانِ', 'تانِك', 'تِه', 'تِي', 'تَيْنِ', 'ثمّ', 'ثمّة', 'ذانِ', 'ذِه', 'ذِي', 'ذَيْنِ', 'هَؤلاء', 'هَاتانِ', 'هَاتِه', 'هَاتِي', 'هَاتَيْنِ', 'هَذا', 'هَذانِ', 'هَذِه', 'هَذِي', 'هَذَيْنِ', 'الألى', 'الألاء', 'أل', 'أنّى', 'أيّ', 'ّأيّان', 'أنّى', 'أيّ', 'ّأيّان', 'ذيت', 'كأيّ', 'كأيّن', 'بضع', 'فلان', 'وا', 'آمينَ', 'آهِ', 'آهٍ', 'آهاً', 'أُفٍّ', 'أُفٍّ', 'أفٍّ', 'أمامك', 'أمامكَ', 'أوّهْ', 'إلَيْكَ', 'إلَيْكَ', 'إليكَ', 'إليكنّ', 'إيهٍ', 'بخٍ', 'بسّ', 'بَسْ', 'بطآن', 'بَلْهَ', 'حاي', 'حَذارِ', 'حيَّ', 'حيَّ', 'دونك', 'رويدك', 'سرعان', 'شتانَ', 'شَتَّانَ', 'صهْ', 'صهٍ', 'طاق', 'طَق', 'عَدَسْ', 'كِخ', 'مكانَك', 'مكانَك', 'مكانَك', 'مكانكم', 'مكانكما', 'مكانكنّ', 'نَخْ', 'هاكَ', 'هَجْ', 'هلم', 'هيّا', 'هَيْهات', 'وا', 'واهاً', 'وراءَك', 'وُشْكَانَ', 'وَيْ', 'يفعلان', 'تفعلان', 'يفعلون', 'تفعلون', 'تفعلين', 'اتخذ', 'ألفى', 'تخذ', 'ترك', 'تعلَّم', 'جعل', 'حجا', 'حبيب', 'خال', 'حسب', 'خال', 'درى', 'رأى', 'زعم', 'صبر', 'ظنَّ', 'عدَّ', 'علم', 'غادر', 'ذهب', 'وجد', 'ورد', 'وهب', 'أسكن', 'أطعم', 'أعطى', 'رزق', 'زود', 'سقى', 'كسا', 'أخبر', 'أرى', 'أعلم', 'أنبأ', 'حدَث', 'خبَّر', 'نبَّا', 'أفعل به', 'ما أفعله', 'بئس', 'ساء', 'طالما', 'قلما', 'لات', 'لكنَّ', 'ءَ', 'أجل', 'إذاً', 'أمّا', 'إمّا', 'إنَّ', 'أنًّ', 'أى', 'إى', 'أيا', 'ب', 'ثمَّ', 'جلل', 'جير', 'رُبَّ', 'س', 'علًّ', 'ف', 'كأنّ', 'كلَّا', 'كى', 'ل', 'لات', 'لعلَّ', 'لكنَّ', 'لكنَّ', 'م', 'نَّ', 'هلّا', 'وا', 'أل', 'إلّا', 'ت', 'ك', 'لمّا', 'ن', 'ه', 'و', 'ا', 'ي', 'تجاه', 'تلقاء', 'جميع', 'حسب', 'سبحان', 'شبه', 'لعمر', 'مثل', 'معاذ', 'أبو', 'أخو', 'حمو', 'فو', 'مئة', 'مئتان', 'ثلاثمئة', 'أربعمئة', 'خمسمئة', 'ستمئة', 'سبعمئة', 'ثمنمئة', 'تسعمئة', 'مائة', 'ثلاثمائة', 'أربعمائة', 'خمسمائة', 'ستمائة', 'سبعمائة', 'ثمانمئة', 'تسعمائة', 'عشرون', 'ثلاثون', 'اربعون', 'خمسون', 'ستون', 'سبعون', 'ثمانون', 'تسعون', 'عشرين', 'ثلاثين', 'اربعين', 'خمسين', 'ستين', 'سبعين', 'ثمانين', 'تسعين', 'بضع', 'نيف', 'أجمع', 'جميع', 'عامة', 'عين', 'نفس', 'لا سيما', 'أصلا', 'أهلا', 'أيضا', 'بؤسا', 'بعدا', 'بغتة', 'تعسا', 'حقا', 'حمدا', 'خلافا', 'خاصة', 'دواليك', 'سحقا', 'سرا', 'سمعا', 'صبرا', 'صدقا', 'صراحة', 'طرا', 'عجبا', 'عيانا', 'غالبا', 'فرادى', 'فضلا', 'قاطبة', 'كثيرا', 'لبيك', 'معاذ', 'أبدا', 'إزاء', 'أصلا', 'الآن', 'أمد', 'أمس', 'آنفا', 'آناء', 'أنّى', 'أول', 'أيّان', 'تارة', 'ثمّ', 'ثمّة', 'حقا', 'صباح', 'مساء', 'ضحوة', 'عوض', 'غدا', 'غداة', 'قطّ', 'كلّما', 'لدن', 'لمّا', 'مرّة', 'قبل', 'خلف', 'أمام', 'فوق', 'تحت', 'يمين', 'شمال', 'ارتدّ', 'استحال', 'أصبح', 'أضحى', 'آض', 'أمسى', 'انقلب', 'بات', 'تبدّل', 'تحوّل', 'حار', 'رجع', 'راح', 'صار', 'ظلّ', 'عاد', 'غدا', 'كان', 'ما انفك', 'ما برح', 'مادام', 'مازال', 'مافتئ', 'ابتدأ', 'أخذ', 'اخلولق', 'أقبل', 'انبرى', 'أنشأ', 'أوشك', 'جعل', 'حرى', 'شرع', 'طفق', 'علق', 'قام', 'كرب', 'كاد', 'هبّ']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "7LJZsDtrFInL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6b684c34"
      },
      "outputs": [],
      "source": [
        "#  permet d’éliminer tous les caractères Tatweel \"_\"\n",
        "def replace_tatweel(text):\n",
        "    TATWEEL = u'\\u0640'\n",
        "    text_list = list(text.split())\n",
        "    new_text_list = text_list.copy()\n",
        "    for (index,word) in enumerate(text_list):\n",
        "          if re.search(TATWEEL, word):\n",
        "              word_index = new_text_list.index(word, index)\n",
        "              new_text_list.insert(word_index + 1, 'جدا')\n",
        "    text = ' '.join(new_text_list)\n",
        "    return re.sub(u'[%s]' % TATWEEL, '', text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "524625df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "9444f9c9-6533-41c0-b88d-55838152dc2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[2023-05-28 08:38:09,713 - farasapy_logger - WARNING]: Be careful with large lines as they may break on interactive mode. You may switch to Standalone mode for such cases.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nvery important note:\\n    The order of the execution of the these function is extremely crucial.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 83
        }
      ],
      "source": [
        "\n",
        "\n",
        "COMMA = u'\\u060C'\n",
        "SEMICOLON = u'\\u061B'\n",
        "QUESTION = u'\\u061F'\n",
        "HAMZA = u'\\u0621'\n",
        "ALEF_MADDA = u'\\u0622'\n",
        "ALEF_HAMZA_ABOVE = u'\\u0623'\n",
        "WAW_HAMZA = u'\\u0624'\n",
        "ALEF_HAMZA_BELOW = u'\\u0625'\n",
        "YEH_HAMZA = u'\\u0626'\n",
        "ALEF = u'\\u0627'\n",
        "BEH = u'\\u0628'\n",
        "TEH_MARBUTA = u'\\u0629'\n",
        "TEH = u'\\u062a'\n",
        "THEH = u'\\u062b'\n",
        "JEEM = u'\\u062c'\n",
        "HAH = u'\\u062d'\n",
        "KHAH = u'\\u062e'\n",
        "DAL = u'\\u062f'\n",
        "THAL = u'\\u0630'\n",
        "REH = u'\\u0631'\n",
        "ZAIN = u'\\u0632'\n",
        "SEEN = u'\\u0633'\n",
        "SHEEN = u'\\u0634'\n",
        "SAD = u'\\u0635'\n",
        "DAD = u'\\u0636'\n",
        "TAH = u'\\u0637'\n",
        "ZAH = u'\\u0638'\n",
        "AIN = u'\\u0639'\n",
        "GHAIN = u'\\u063a'\n",
        "TATWEEL = u'\\u0640'\n",
        "FEH = u'\\u0641'\n",
        "QAF = u'\\u0642'\n",
        "KAF = u'\\u0643'\n",
        "LAM = u'\\u0644'\n",
        "MEEM = u'\\u0645'\n",
        "NOON = u'\\u0646'\n",
        "HEH = u'\\u0647'\n",
        "WAW = u'\\u0648'\n",
        "ALEF_MAKSURA = u'\\u0649'\n",
        "YEH = u'\\u064a'\n",
        "MADDA_ABOVE = u'\\u0653'\n",
        "HAMZA_ABOVE = u'\\u0654'\n",
        "HAMZA_BELOW = u'\\u0655'\n",
        "ZERO = u'\\u0660'\n",
        "ONE = u'\\u0661'\n",
        "TWO = u'\\u0662'\n",
        "THREE = u'\\u0663'\n",
        "FOUR = u'\\u0664'\n",
        "FIVE = u'\\u0665'\n",
        "SIX = u'\\u0666'\n",
        "SEVEN = u'\\u0667'\n",
        "EIGHT = u'\\u0668'\n",
        "NINE = u'\\u0669'\n",
        "PERCENT = u'\\u066a'\n",
        "DECIMAL = u'\\u066b'\n",
        "THOUSANDS = u'\\u066c'\n",
        "STAR = u'\\u066d'\n",
        "MINI_ALEF = u'\\u0670'\n",
        "ALEF_WASLA = u'\\u0671'\n",
        "FULL_STOP = u'\\u06d4'\n",
        "BYTE_ORDER_MARK = u'\\ufeff'\n",
        "\n",
        "# Diacritics\n",
        "FATHATAN = u'\\u064b'\n",
        "DAMMATAN = u'\\u064c'\n",
        "KASRATAN = u'\\u064d'\n",
        "FATHA = u'\\u064e'\n",
        "DAMMA = u'\\u064f'\n",
        "KASRA = u'\\u0650'\n",
        "SHADDA = u'\\u0651'\n",
        "SUKUN = u'\\u0652'\n",
        "\n",
        "#Ligatures\n",
        "LAM_ALEF = u'\\ufefb'\n",
        "LAM_ALEF_HAMZA_ABOVE = u'\\ufef7'\n",
        "LAM_ALEF_HAMZA_BELOW = u'\\ufef9'\n",
        "LAM_ALEF_MADDA_ABOVE = u'\\ufef5'\n",
        "SIMPLE_LAM_ALEF = u'\\u0644\\u0627'\n",
        "SIMPLE_LAM_ALEF_HAMZA_ABOVE = u'\\u0644\\u0623'\n",
        "SIMPLE_LAM_ALEF_HAMZA_BELOW = u'\\u0644\\u0625'\n",
        "SIMPLE_LAM_ALEF_MADDA_ABOVE = u'\\u0644\\u0622'\n",
        "\n",
        "\n",
        "HARAKAT_PAT = re.compile(u\"[\"+u\"\".join([FATHATAN, DAMMATAN, KASRATAN,\n",
        "                                        FATHA, DAMMA, KASRA, SUKUN,\n",
        "                                        SHADDA])+u\"]\")\n",
        "HAMZAT_PAT = re.compile(u\"[\"+u\"\".join([WAW_HAMZA, YEH_HAMZA])+u\"]\")\n",
        "ALEFAT_PAT = re.compile(u\"[\"+u\"\".join([ALEF_MADDA, ALEF_HAMZA_ABOVE,\n",
        "                                       ALEF_HAMZA_BELOW, HAMZA_ABOVE,\n",
        "                                       HAMZA_BELOW])+u\"]\")\n",
        "LAMALEFAT_PAT = re.compile(u\"[\"+u\"\".join([LAM_ALEF,\n",
        "                                          LAM_ALEF_HAMZA_ABOVE,\n",
        "                                          LAM_ALEF_HAMZA_BELOW,\n",
        "LAM_ALEF_MADDA_ABOVE])+u\"]\")\n",
        "\n",
        "\n",
        "WESTERN_ARABIC_NUMERALS = ['0','1','2','3','4','5','6','7','8','9']\n",
        "\n",
        "#EASTERN_ARABIC_NUMERALS = [u'\\u06F0', u'\\u06F1', u'\\u06F2', u'\\u06F3', u'\\u0664', u'\\u06F5', u'\\u0666', u'\\u06F7', u'\\u06F8', u'\\u06F9']\n",
        "EASTERN_ARABIC_NUMERALS = [u'۰', u'۱', u'۲', u'۳', u'٤', u'۵', u'٦', u'۷', u'۸', u'۹']\n",
        "\n",
        "eastern_to_western_numerals = {}\n",
        "for i in range(len(EASTERN_ARABIC_NUMERALS)):\n",
        "    eastern_to_western_numerals[EASTERN_ARABIC_NUMERALS[i]] = WESTERN_ARABIC_NUMERALS[i]\n",
        "\n",
        "# Punctuation marks\n",
        "COMMA = u'\\u060C'\n",
        "SEMICOLON = u'\\u061B'\n",
        "QUESTION = u'\\u061F'\n",
        "\n",
        "# Other symbols\n",
        "PERCENT = u'\\u066a'\n",
        "DECIMAL = u'\\u066b'\n",
        "THOUSANDS = u'\\u066c'\n",
        "STAR = u'\\u066d'\n",
        "FULL_STOP = u'\\u06d4'\n",
        "MULITIPLICATION_SIGN = u'\\u00D7'\n",
        "DIVISION_SIGN = u'\\u00F7'\n",
        "\n",
        "arabic_punctuations = COMMA + SEMICOLON + QUESTION + PERCENT + DECIMAL + THOUSANDS + STAR + FULL_STOP + MULITIPLICATION_SIGN + DIVISION_SIGN\n",
        "all_punctuations = string.punctuation + arabic_punctuations + '()[]{}'\n",
        "\n",
        "all_punctuations = ''.join(list(set(all_punctuations)))\n",
        "\n",
        "\n",
        "\n",
        "stemmer = FarasaStemmer(interactive=True)\n",
        "\n",
        "# La fonction strip_diacritics() permet de prétraiter un texte en éliminant les diacri\u0002tiques arabes\n",
        "def strip_diacritics(text):\n",
        "    text = HARAKAT_PAT.sub('', text)\n",
        "    text = re.sub(u\"[\\u064E]\", \"\", text,  flags=re.UNICODE) # fat'ha\n",
        "    text = re.sub(u\"[\\u0671]\", \"\", text,  flags=re.UNICODE) # wasla\n",
        "    return text\n",
        "\n",
        "\n",
        "def strip_tatweel(text):\n",
        "    return re.sub(u'[%s]' % TATWEEL, '', text)\n",
        "\n",
        "# permet de supprimer tous les caractères non arabes\n",
        "def remove_non_arabic(text):\n",
        "    return ' '.join(re.sub(u\"[^\\u0621-\\u063A\\u0640-\\u0652 ]\", \" \", text,  flags=re.UNICODE).split())\n",
        "\n",
        "\n",
        "def keep_arabic_english_n_symbols(text):\n",
        "    return ' '.join(re.sub(u\"[^\\u0621-\\u063A\\u0640-\\u0652a-zA-Z#@_:/ ]\", \" \", text,  flags=re.UNICODE).split())\n",
        "\n",
        "# e normaliser les différentes formes de la lettre Hamza en arabe\n",
        "def normalize_hamza(text):\n",
        "    text = ALEFAT_PAT.sub(ALEF, text)\n",
        "    return HAMZAT_PAT.sub(HAMZA, text)\n",
        "\n",
        "\n",
        "def normalize_spellerrors(text):\n",
        "    text = re.sub(u'[%s]' % TEH_MARBUTA, HEH, text)\n",
        "    return re.sub(u'[%s]' % ALEF_MAKSURA, YEH, text)\n",
        "\n",
        "\n",
        "def normalize_lamalef(text):\n",
        "    return LAMALEFAT_PAT.sub(u'%s%s'%(LAM, ALEF), text)\n",
        "\n",
        "\n",
        "def normalize_arabic_text(text):\n",
        "    text = remove_non_arabic(text)\n",
        "    text = strip_diacritics(text)\n",
        "    text = strip_tatweel(text)\n",
        "\n",
        "    text = stemmer.stem(text)\n",
        "\n",
        "    text = normalize_lamalef(text)\n",
        "    text = normalize_hamza(text)\n",
        "    text = normalize_spellerrors(text)\n",
        "    return text\n",
        "\n",
        "# supprimer les underscores \"_\"\n",
        "def remove_underscore(text):\n",
        "    return ' '.join(text.split('_'))\n",
        "\n",
        "def replace_emails(text):\n",
        "    emails = re.findall(r'[\\w\\.-]+@[\\w\\.-]+', text)\n",
        "    for email in emails:\n",
        "        text = text.replace(email,' ايميل هنا ')\n",
        "        #text = text.replace(email,' hasEmailAddress ')\n",
        "    return text\n",
        "\n",
        "def replace_urls(text):\n",
        "    return re.sub(r\"http\\S+|www.\\S+\", \" رابط هنا \", text)\n",
        "    #return re.sub(r\"http\\S+|www.\\S+\", \" hasURL \", text)\n",
        "\n",
        "# convertir les numéraux arabes orientaux en numéraux arabes occidentaux\n",
        "def convert_eastern_to_western_numerals(text):\n",
        "    for num in EASTERN_ARABIC_NUMERALS:\n",
        "        text = text.replace(num, eastern_to_western_numerals[num])\n",
        "    return text\n",
        "\n",
        "def remove_all_punctuations(text):\n",
        "    for punctuation in all_punctuations:\n",
        "        text = text.replace(punctuation, ' ')\n",
        "    return text\n",
        "\n",
        "def remove_emojis(text):\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', text)\n",
        "\n",
        "def replace_phone_numbers(text):\n",
        "    return re.sub(r'\\d{10}', ' رقم هاتف هنا ', text)\n",
        "    # return re.sub(r'\\d{10}', ' hasPhoneNumber ', text)\n",
        "\n",
        "# remove_extra_spaces\n",
        "def remove_extra_spaces(text):\n",
        "    return ' '.join(text.split())\n",
        "\n",
        "def remove_stopwords(text_in):\n",
        "    #stopwords = stopwords_list\n",
        "    text_out = ' '.join(filter(lambda word: word not in stopwords_list, text_in.split(' ')))\n",
        "    return text_out\n",
        "\n",
        "'''\n",
        "very important note:\n",
        "    The order of the execution of the these function is extremely crucial.\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b5449b4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "846af34f-36a9-41c8-aa38-52e1f1113b2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "سيئ و فضيع\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'def remove_stopwords2(text):\\n\\n    rem_ele = [   \\'لسنا\\',\\'لستما\\',\\'سوي\\',\\'لكنما\\',\\'غير\\',\\'لكن\\',\\'أقل\\',\\'أكثر\\',\\'إلا\\',\\'بؤسا\\', \\'بئس\\',\\'لا\\',  \\'ليس\\',\\'لم\\',\\'ليست\\', \\'ليسوا\\',\\'لن\\']\\n    \\n    new_stopwords= set(stopwords_list) - set(rem_ele)\\n\\n    processed_word_list = []\\n    word_list=word_tokenize(text) \\n\\n    for word in word_list:\\n        if word not in list(new_stopwords):\\n            processed_word_list.append(word)\\n\\n    return \" \".join(processed_word_list)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 84
        }
      ],
      "source": [
        "# sup les lettres redon\u0002dantes dans un text\n",
        "def replace_redundant_lettersv2(text):\n",
        "    text_list = list(text.split())\n",
        "    new_text_list = text_list.copy()\n",
        "    new_list = []\n",
        "\n",
        "    # sentence = nltk.tokenize.word_tokenize(text)\n",
        "    # print(sentence)\n",
        "    # tokens = nltk.tokenize.wordpunct_tokenize(text)\n",
        "    # tokenpos = nltk.pos_tag(tokens)\n",
        "\n",
        "    for (index,word) in enumerate(text_list):\n",
        "      new_word = re.sub(\"(.)\\\\1{2,}\", \"\\\\1\", word)\n",
        "      new_list.append(new_word)\n",
        "\n",
        "      # if ('JJ' in tokenpos[index][1]):\n",
        "      #   new_list.append('جدا')\n",
        "\n",
        "    # sentence = nltk.tokenize.sent_tokenize(' '.join(new_list))\n",
        "    # token = nltk.tokenize.wordpunct_tokenize(sentence[0])\n",
        "    # tokenpos = pos_tagger.tag(token)\n",
        "    # for index, word in enumerate(tokenpos):\n",
        "    #   if ('JJ' in word[1] or 'NNP' in word[1]):\n",
        "    #     new_list.append('جدا')\n",
        "\n",
        "    text = ' '.join(new_list)\n",
        "    return text\n",
        "\n",
        "\n",
        "smpl_text = ''' سيييييئ و فضيع'''\n",
        "print(replace_redundant_lettersv2(smpl_text))\n",
        "from nltk.tokenize import word_tokenize # ,sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# def remove_stopwords2(text):\n",
        "#     stopwords_list\n",
        "#     processed_word_list = []\n",
        "#     word_list = word_tokenize(text)\n",
        "\n",
        "#     for word in word_list:\n",
        "#         if word.lower() not in stopwords_list:\n",
        "#             processed_word_list.append(word)\n",
        "\n",
        "#     return \" \".join(processed_word_list)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3fbaa45d"
      },
      "outputs": [],
      "source": [
        "def normalize_text(text):\n",
        "    new_text = replace_tatweel(text)\n",
        "    new_text = strip_diacritics(new_text)\n",
        "    new_text = strip_tatweel(new_text)\n",
        "\n",
        "    new_text = remove_non_arabic(new_text)\n",
        "    new_text = keep_arabic_english_n_symbols(new_text)\n",
        "    new_text = normalize_hamza(new_text)\n",
        "    new_text = normalize_spellerrors(new_text)\n",
        "    new_text = normalize_lamalef(new_text)\n",
        "    new_text = normalize_arabic_text(text)\n",
        "    new_text = remove_underscore(new_text)\n",
        "    new_text = replace_emails(new_text)\n",
        "    new_text = replace_urls(new_text)\n",
        "    new_text = convert_eastern_to_western_numerals(new_text)\n",
        "    new_text = remove_all_punctuations(new_text)\n",
        "    new_text = replace_phone_numbers(new_text)\n",
        "    new_text = remove_extra_spaces(new_text)\n",
        "    new_text = replace_redundant_lettersv2(new_text)\n",
        "    new_text = remove_stopwords(new_text)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    #new_text = segmenter.segment(new_text)\n",
        "    return new_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3bfa86b2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "72ee57d2"
      },
      "outputs": [],
      "source": [
        "def process_text(x):\n",
        "  text = x['text']\n",
        "  processed_text = normalize_text(text)\n",
        "  x['text'] = processed_text\n",
        "  return x"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def df_process(dataset):\n",
        "  return dataset.apply(process_text, axis=1)"
      ],
      "metadata": {
        "id": "MNDsJyMGUT8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "row = df.iloc[4419]\n",
        "print(row['text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DmugaOYJgXR2",
        "outputId": "954d91cb-3731-4255-f75e-c704559c755b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "الاستقبال فخم وبالرغم من حضورنا قبل موعد استلام الغرفه ولانها لازالت مأهوله الا انهم مشكورين قدموا لنا غرفة مؤقته لنقيم فيها حتى تخلو الغرفه من ساكنيها\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_df = df_process(df)"
      ],
      "metadata": {
        "id": "Na7D7QyTzwKo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "# Access the row at index 2032\n",
        "row = df.iloc[4419]\n",
        "row2 = new_df.iloc[4419]\n",
        "# Print the text field\n",
        "print(row['text'])\n",
        "print(row2['text'])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kYBSAelG0t9x",
        "outputId": "92eb17b0-8be1-4e68-da72-365d7ce4dbca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "الاستقبال فخم وبالرغم من حضورنا قبل موعد استلام الغرفه ولانها لازالت مأهوله الا انهم مشكورين قدموا لنا غرفة مؤقته لنقيم فيها حتى تخلو الغرفه من ساكنيها\n",
            "استقبال فخم رغم حضور موعد استلام غرفه ان ازال ماهول الا ان مشكور قدم غرفه مءقت اقام حتي غرفه ساكن\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(stopwords_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nM657Axr48PD",
        "outputId": "cd58400a-cbb0-4c07-9191-999ea49470fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['إذ', 'إذا', 'إذما', 'إذن', 'أف', 'أقل', 'أكثر', 'ألا', 'إلا', 'التي', 'الذي', 'الذين', 'اللاتي', 'اللائي', 'اللتان', 'اللتيا', 'اللتين', 'اللذان', 'اللذين', 'اللواتي', 'إلى', 'إليك', 'إليكم', 'إليكما', 'إليكن', 'أم', 'أما', 'أما', 'إما', 'أن', 'إن', 'إنا', 'أنا', 'أنت', 'أنتم', 'أنتما', 'أنتن', 'إنما', 'إنه', 'أنى', 'أنى', 'آه', 'آها', 'أو', 'أولاء', 'أولئك', 'أوه', 'آي', 'أي', 'أيها', 'إي', 'أين', 'أين', 'أينما', 'إيه', 'بخ', 'بس', 'بعد', 'بعض', 'بك', 'بكم', 'بكم', 'بكما', 'بكن', 'بل', 'بلى', 'بما', 'بماذا', 'بمن', 'بنا', 'به', 'بها', 'بهم', 'بهما', 'بهن', 'بي', 'بين', 'بيد', 'تلك', 'تلكم', 'تلكما', 'ته', 'تي', 'تين', 'تينك', 'ثم', 'ثمة', 'حاشا', 'حبذا', 'حتى', 'حيث', 'حيثما', 'حين', 'خلا', 'دون', 'ذا', 'ذات', 'ذاك', 'ذان', 'ذانك', 'ذلك', 'ذلكم', 'ذلكما', 'ذلكن', 'ذه', 'ذو', 'ذوا', 'ذواتا', 'ذواتي', 'ذي', 'ذين', 'ذينك', 'ريث', 'سوف', 'سوى', 'شتان', 'عدا', 'عسى', 'عل', 'على', 'عليك', 'عليه', 'عما', 'عن', 'عند', 'غير', 'فإذا', 'فإن', 'فلا', 'فمن', 'في', 'فيم', 'فيما', 'فيه', 'فيها', 'قد', 'كأن', 'كأنما', 'كأي', 'كأين', 'كذا', 'كذلك', 'كل', 'كلا', 'كلاهما', 'كلتا', 'كلما', 'كليكما', 'كليهما', 'كم', 'كم', 'كما', 'كي', 'كيت', 'كيف', 'كيفما', 'لا', 'لاسيما', 'لدى', 'لست', 'لستم', 'لستما', 'لستن', 'لسن', 'لسنا', 'لعل', 'لك', 'لكم', 'لكما', 'لكن', 'لكنما', 'لكي', 'لكيلا', 'لم', 'لما', 'لن', 'لنا', 'له', 'لها', 'لهم', 'لهما', 'لهن', 'لو', 'لولا', 'لوما', 'لي', 'لئن', 'ليت', 'ليس', 'ليسا', 'ليست', 'ليستا', 'ليسوا', 'ما', 'ماذا', 'متى', 'مذ', 'مع', 'مما', 'ممن', 'من', 'منه', 'منها', 'منذ', 'مه', 'مهما', 'نحن', 'نحو', 'نعم', 'ها', 'هاتان', 'هاته', 'هاتي', 'هاتين', 'هاك', 'هاهنا', 'هذا', 'هذان', 'هذه', 'هذي', 'هذين', 'هكذا', 'هل', 'هلا', 'هم', 'هما', 'هن', 'هنا', 'هناك', 'هنالك', 'هو', 'هؤلاء', 'هي', 'هيا', 'هيت', 'هيهات', 'والذي', 'والذين', 'وإذ', 'وإذا', 'وإن', 'ولا', 'ولكن', 'ولو', 'وما', 'ومن', 'وهو', 'يا', 'أبٌ', 'أخٌ', 'حمٌ', 'فو', 'أنتِ', 'يناير', 'فبراير', 'مارس', 'أبريل', 'مايو', 'يونيو', 'يوليو', 'أغسطس', 'سبتمبر', 'أكتوبر', 'نوفمبر', 'ديسمبر', 'جانفي', 'فيفري', 'مارس', 'أفريل', 'ماي', 'جوان', 'جويلية', 'أوت', 'كانون', 'شباط', 'آذار', 'نيسان', 'أيار', 'حزيران', 'تموز', 'آب', 'أيلول', 'تشرين', 'دولار', 'دينار', 'ريال', 'درهم', 'ليرة', 'جنيه', 'قرش', 'مليم', 'فلس', 'هللة', 'سنتيم', 'يورو', 'ين', 'يوان', 'شيكل', 'واحد', 'اثنان', 'ثلاثة', 'أربعة', 'خمسة', 'ستة', 'سبعة', 'ثمانية', 'تسعة', 'عشرة', 'أحد', 'اثنا', 'اثني', 'إحدى', 'ثلاث', 'أربع', 'خمس', 'ست', 'سبع', 'ثماني', 'تسع', 'عشر', 'ثمان', 'سبت', 'أحد', 'اثنين', 'ثلاثاء', 'أربعاء', 'خميس', 'جمعة', 'أول', 'ثان', 'ثاني', 'ثالث', 'رابع', 'خامس', 'سادس', 'سابع', 'ثامن', 'تاسع', 'عاشر', 'حادي', 'أ', 'ب', 'ت', 'ث', 'ج', 'ح', 'خ', 'د', 'ذ', 'ر', 'ز', 'س', 'ش', 'ص', 'ض', 'ط', 'ظ', 'ع', 'غ', 'ف', 'ق', 'ك', 'ل', 'م', 'ن', 'ه', 'و', 'ي', 'ء', 'ى', 'آ', 'ؤ', 'ئ', 'أ', 'ة', 'ألف', 'باء', 'تاء', 'ثاء', 'جيم', 'حاء', 'خاء', 'دال', 'ذال', 'راء', 'زاي', 'سين', 'شين', 'صاد', 'ضاد', 'طاء', 'ظاء', 'عين', 'غين', 'فاء', 'قاف', 'كاف', 'لام', 'ميم', 'نون', 'هاء', 'واو', 'ياء', 'همزة', 'ي', 'نا', 'ك', 'كن', 'ه', 'إياه', 'إياها', 'إياهما', 'إياهم', 'إياهن', 'إياك', 'إياكما', 'إياكم', 'إياك', 'إياكن', 'إياي', 'إيانا', 'أولالك', 'تانِ', 'تانِك', 'تِه', 'تِي', 'تَيْنِ', 'ثمّ', 'ثمّة', 'ذانِ', 'ذِه', 'ذِي', 'ذَيْنِ', 'هَؤلاء', 'هَاتانِ', 'هَاتِه', 'هَاتِي', 'هَاتَيْنِ', 'هَذا', 'هَذانِ', 'هَذِه', 'هَذِي', 'هَذَيْنِ', 'الألى', 'الألاء', 'أل', 'أنّى', 'أيّ', 'ّأيّان', 'أنّى', 'أيّ', 'ّأيّان', 'ذيت', 'كأيّ', 'كأيّن', 'بضع', 'فلان', 'وا', 'آمينَ', 'آهِ', 'آهٍ', 'آهاً', 'أُفٍّ', 'أُفٍّ', 'أفٍّ', 'أمامك', 'أمامكَ', 'أوّهْ', 'إلَيْكَ', 'إلَيْكَ', 'إليكَ', 'إليكنّ', 'إيهٍ', 'بخٍ', 'بسّ', 'بَسْ', 'بطآن', 'بَلْهَ', 'حاي', 'حَذارِ', 'حيَّ', 'حيَّ', 'دونك', 'رويدك', 'سرعان', 'شتانَ', 'شَتَّانَ', 'صهْ', 'صهٍ', 'طاق', 'طَق', 'عَدَسْ', 'كِخ', 'مكانَك', 'مكانَك', 'مكانَك', 'مكانكم', 'مكانكما', 'مكانكنّ', 'نَخْ', 'هاكَ', 'هَجْ', 'هلم', 'هيّا', 'هَيْهات', 'وا', 'واهاً', 'وراءَك', 'وُشْكَانَ', 'وَيْ', 'يفعلان', 'تفعلان', 'يفعلون', 'تفعلون', 'تفعلين', 'اتخذ', 'ألفى', 'تخذ', 'ترك', 'تعلَّم', 'جعل', 'حجا', 'حبيب', 'خال', 'حسب', 'خال', 'درى', 'رأى', 'زعم', 'صبر', 'ظنَّ', 'عدَّ', 'علم', 'غادر', 'ذهب', 'وجد', 'ورد', 'وهب', 'أسكن', 'أطعم', 'أعطى', 'رزق', 'زود', 'سقى', 'كسا', 'أخبر', 'أرى', 'أعلم', 'أنبأ', 'حدَث', 'خبَّر', 'نبَّا', 'أفعل به', 'ما أفعله', 'بئس', 'ساء', 'طالما', 'قلما', 'لات', 'لكنَّ', 'ءَ', 'أجل', 'إذاً', 'أمّا', 'إمّا', 'إنَّ', 'أنًّ', 'أى', 'إى', 'أيا', 'ب', 'ثمَّ', 'جلل', 'جير', 'رُبَّ', 'س', 'علًّ', 'ف', 'كأنّ', 'كلَّا', 'كى', 'ل', 'لات', 'لعلَّ', 'لكنَّ', 'لكنَّ', 'م', 'نَّ', 'هلّا', 'وا', 'أل', 'إلّا', 'ت', 'ك', 'لمّا', 'ن', 'ه', 'و', 'ا', 'ي', 'تجاه', 'تلقاء', 'جميع', 'حسب', 'سبحان', 'شبه', 'لعمر', 'مثل', 'معاذ', 'أبو', 'أخو', 'حمو', 'فو', 'مئة', 'مئتان', 'ثلاثمئة', 'أربعمئة', 'خمسمئة', 'ستمئة', 'سبعمئة', 'ثمنمئة', 'تسعمئة', 'مائة', 'ثلاثمائة', 'أربعمائة', 'خمسمائة', 'ستمائة', 'سبعمائة', 'ثمانمئة', 'تسعمائة', 'عشرون', 'ثلاثون', 'اربعون', 'خمسون', 'ستون', 'سبعون', 'ثمانون', 'تسعون', 'عشرين', 'ثلاثين', 'اربعين', 'خمسين', 'ستين', 'سبعين', 'ثمانين', 'تسعين', 'بضع', 'نيف', 'أجمع', 'جميع', 'عامة', 'عين', 'نفس', 'لا سيما', 'أصلا', 'أهلا', 'أيضا', 'بؤسا', 'بعدا', 'بغتة', 'تعسا', 'حقا', 'حمدا', 'خلافا', 'خاصة', 'دواليك', 'سحقا', 'سرا', 'سمعا', 'صبرا', 'صدقا', 'صراحة', 'طرا', 'عجبا', 'عيانا', 'غالبا', 'فرادى', 'فضلا', 'قاطبة', 'كثيرا', 'لبيك', 'معاذ', 'أبدا', 'إزاء', 'أصلا', 'الآن', 'أمد', 'أمس', 'آنفا', 'آناء', 'أنّى', 'أول', 'أيّان', 'تارة', 'ثمّ', 'ثمّة', 'حقا', 'صباح', 'مساء', 'ضحوة', 'عوض', 'غدا', 'غداة', 'قطّ', 'كلّما', 'لدن', 'لمّا', 'مرّة', 'قبل', 'خلف', 'أمام', 'فوق', 'تحت', 'يمين', 'شمال', 'ارتدّ', 'استحال', 'أصبح', 'أضحى', 'آض', 'أمسى', 'انقلب', 'بات', 'تبدّل', 'تحوّل', 'حار', 'رجع', 'راح', 'صار', 'ظلّ', 'عاد', 'غدا', 'كان', 'ما انفك', 'ما برح', 'مادام', 'مازال', 'مافتئ', 'ابتدأ', 'أخذ', 'اخلولق', 'أقبل', 'انبرى', 'أنشأ', 'أوشك', 'جعل', 'حرى', 'شرع', 'طفق', 'علق', 'قام', 'كرب', 'كاد', 'هبّ']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_df.to_excel('/content/gdrive/MyDrive/dataxlsx/new_dataset.xlsx', index=False)"
      ],
      "metadata": {
        "id": "SP6YcaBC5ewM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}